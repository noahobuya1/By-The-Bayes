---
title: "By the Bayes (STA240 Project)"
author: "Noah Obuya & Tamya Davidson"
date: "2024-05-01"
output:
  pdf_document:
    keep_tex: yes
    latex_engine: xelatex
  html_document: default
---

```{r initializing, echo=FALSE, warning=FALSE, message=FALSE}

library(tidyverse)
library(knitr)
library(rmarkdown)
library(pandoc)

```

# Proposal

## Introduction

Welcome to By the Bayes! A restaurant serving food straight from the Louisiana Bayou. What makes our restaurant special is that every step of our process is backed by probabilistic modeling and inference. It's in our name. Today we'll be taking you through the customer's queuing experience in two different scenarios, providing simulations and explanations to back all of our claims.

## Analysis Plan

### Scenario One

Our first scenario models simple setting with one dining table and one chef, with operating hours 10am - 10pm. Suppose customers arrive according to a Poisson process with a rate of

$$
 \lambda_A = 5 \ \text{{per hour}} , \ \text{40 total customers from open to close}
$$

Once a customer arrives, their total service time (ordering, cooking and eating) can be modeled by an exponential distribution with rate λS = 6.

$$
 \lambda_S = 6 
$$

To get a sense of the customer queuing experience, we will run multiple simulations of our exponential distribution. The simulation will model the time it takes to service one customer. Essentially, once one customer comes in, we want to know how long it takes for another customer to be serviced. We only have one table and one chef, so we are only able to handle one customer at a time. We will build upon the exponential distribution, providing two models, as we continue to model its efficacy.

### Scenario Two

Our second scenario gives us 5 dining tables and $$L$$ chefs, with operating hours 10am - 10pm. Customers arrive according to a Poisson process with a rate of

$$
 \lambda_A = 10 \ \text{{per hour}} , \ \text{80 total customers from open to close}
$$

Once a customer arrives, their total service time can be modeled by an exponential distribution with rate λS = 3L. Each additional chef cuts down the wait time by $\frac 13$.

$$
 \lambda_S = 3L
$$

We are assuming a customer spends \$50 per meal, and that each chef earns a wage of \$40 per hour.

To simulate this scenario, we will expand upon the model developed in Scenario One with increased tables and an additional input for the number of chefs. We'll use the queueing library in R-Studio, designed to model and analyze queueing systems where tasks or people wait to be processed. This allows us to simulate real-world queuing situations like our own@

Queueing theory employs various models to represent systems, commonly described using Kendall's notation (M/M/c), where 'M' stands for Markovian (memoryless and random), and 'c' represents the number of servers. Our initial model was an M/M/1, indicating a single server or chef. In the current model, 'c' varies as we explore the optimal number of chefs needed.

After running a simulation using queuing theory, we will create a function that, when given the input of chef number, returns to us average wait time, revenue, wages and of course, profits. This final table will allow us to make a decision on how many chefs we should hire.

# Analysis

## Scenario One

#### Part One - The Mean

```{r meanwaittime plot, echo=FALSE, warning=FALSE, message=FALSE, fig.width=4, fig.height=3}


lambda_s1 <- 6

x <- seq(0, 1, length.out = 1000)
y <- dexp(x, rate = lambda_s1)

meanwaitingtime <- data.frame(x = x, y = y)

p <- ggplot(meanwaitingtime, aes(x = x, y = y)) +
  geom_line() +
  geom_vline(aes(xintercept = 1/lambda_s1), linetype = "dashed", color = "red") +
  annotate("text", x = 1/lambda_s1, y = max(meanwaitingtime$y)/2, label = "Mean waiting time", hjust = -0.1, color = "red") +
  labs(x = "Waiting time", y = "Density", title = "Exponential distribution with lambda = 6")

p


```

Figure 1 highlights the mean wait time as indicated by our model's Probability Density Function (PDF), where the average wait time is shown as $\frac{1}{\lambda} = \frac{1}{6}$ hours, or approximately 10 minutes. This model, ideal for initial planning, assumes a constant service rate and single service channel but its memorylessness—constant probability of service completion regardless of waiting time—may not align well with variable real-world customer arrivals.

The observed variability in wait times from the graph, ranging from shorter to unexpectedly longer, illustrates the limitations of the exponential distribution and the importance of employing more comprehensive models like M/M/c in complex settings. This adaptation is crucial for effectively managing customer expectations and optimizing service delivery strategies.


#### Part Two - First Scenario One Simulation

```{r s1simulation1, echo=FALSE, warning=FALSE, message=FALSE}

set.seed(1)

lambda_A <- 5 
lambda_S <- 6
num_customers <- 1000
num_simulations <- 3

run_simulation <- function() {
  arrival_times <- cumsum(rexp(num_customers, rate=lambda_A))
  service_start_times <- pmax(arrival_times, cummax(arrival_times[-1] + rexp(num_customers-1, rate=lambda_S)))
  service_end_times <- service_start_times + rexp(num_customers, rate=lambda_S)
  wait_times <- service_start_times - arrival_times
  return(mean(wait_times))
}

average_wait_times <- replicate(num_simulations, run_simulation())

average_wait_times_df <- data.frame(
  Simulation_Run = 1:num_simulations,
  Average_Wait_Time_hours = average_wait_times,
  Average_Wait_Time_minutes = average_wait_times * 60 
)


overall_average <- mean(average_wait_times)
average_wait_times_df <- rbind(average_wait_times_df, c("Average", overall_average, overall_average * 60))


names(average_wait_times_df)[1] <- "Simulation Run"
names(average_wait_times_df)[2] <- "Average Wait Time (hours)"
names(average_wait_times_df)[3] <- "Average Wait Time (minutes)"


knitr::kable(average_wait_times_df, format = "markdown", caption = "Average Wait Times for Each Simulation Run")





```

The initial simulation attempts to model a single-server queue system where each customer's service and wait time are determined based on exponential distributions for arrivals and services. It assumes random arrivals and service times but lacks detailed dynamics such as the server's operational status (idle or busy) and does not track the length of the queue. This simplistic approach provides basic insights but overlooks several aspects of real-world service environments.

We can demonstrate that this model is missing the mark through observing our means for the queuing simulation

Mean Wait Time in the System (including service time):
$$W = \frac{1}{\mu - \lambda}$$
Mean Wait Time in the Queue (excluding service time):
$$W_q = \frac{\rho}{\mu - \lambda}$$
where $$ \rho = \frac{\lambda}{\mu} $$ represents the server utilization, server utilization being the amount of time a server is "busy". Caculating our two means we get:

$$W = 60 \ minutes$$ and $$W_q = 50 \ minutes$$

So, clearly, our model is incorrectly averaging means, because it finds that customers are only waiting for an average of 25 minutes ($W = 25 \ minutes$)

This model needs to improve by incorporating more realistic customer behaviors and queue dynamics. Enhancements such as considering customer balking or reneging, and providing metrics for queue length and server utilization, would offer a more comprehensive understanding. Additionally, incorporating variability in arrival and service rates to reflect different operational conditions like peak hours would make the simulation more robust.

### Part Three - Second Scenario One Simulation

```{r s1simulation2, echo=FALSE, warning=FALSE, message=FALSE}

set.seed(2)  

lambda_A <- 5  
lambda_S <- 6  
num_customers <- 1000
num_simulations <- 3

run_simulation <- function() {
  arrival_times <- cumsum(rexp(num_customers, rate=lambda_A))
  service_start_times <- numeric(num_customers)
  service_end_times <- numeric(num_customers)
  
  service_start_times[1] <- arrival_times[1]
  service_end_times[1] <- service_start_times[1] + rexp(1, rate=lambda_S)
  
  for (i in 2:num_customers) {
    service_start_times[i] <- max(arrival_times[i], service_end_times[i-1])
    service_end_times[i] <- service_start_times[i] + rexp(1, rate=lambda_S)
  }
  
  wait_times <- service_start_times - arrival_times
  return(mean(wait_times))
}

average_wait_times <- replicate(num_simulations, run_simulation())

average_wait_times_df <- data.frame(
  "Simulation Run" = 1:num_simulations,
  "Average Wait Time (hours)" = average_wait_times,
  "Average Wait Time (minutes)" = average_wait_times * 60  
)

overall_average <- mean(average_wait_times)
average_wait_times_df <- rbind(average_wait_times_df, c("Average", overall_average, overall_average * 60))

names(average_wait_times_df)[1] <- "Simulation Run"
names(average_wait_times_df)[2] <- "Average Wait Time (hours)"
names(average_wait_times_df)[3] <- "Average Wait Time (minutes)"

kable(average_wait_times_df, format = "markdown", caption = "Average Wait Times Improved Simulation")


```

As we can see, the improved simulation averages much closer to the true mean wait time of $W = 60 \ minutes$. The revised simulation significantly improves upon the first by explicitly calculating the waiting times for each customer, considering the time until the previous customer's service is complete. This approach more accurately reflects actual queue dynamics, where service cannot begin until the previous service has concluded, thus providing a realistic measure of waiting times. Each customer's start time is dynamically adjusted based on the service end time of the preceding customer, which prevents any overlap and simulates a true first-come, first-served queue.

This methodology provides deeper insights into how service efficiency impacts customer waiting experiences, making it a valuable tool for operational planning and optimization. Further improvements could include multi-server configurations, which is what we will tackle next, in Scenario Two.

### Scenario Two


```{r Wait Times and Tables, echo=FALSE, warning=FALSE, message=FALSE}

library(queueing)

simulate_day <- function(L) {
  lambda_A <- 10  
  num_tables <- 5  
  lambda_S <- 3 * L  

 
  Q_mm_c <- NewInput.MMC(lambda = lambda_A, mu = lambda_S, c = num_tables, n = 1000)
  result <- QueueingModel(Q_mm_c)
  
  avg_wait_time <- result$Lq / lambda_A * 60  
  
  return(list(avg_wait_time = avg_wait_time, utilization = result$U))
}

chef_values <- 1:10  
wait_times <- numeric(length(chef_values))
utilizations <- numeric(length(chef_values))

for (i in seq_along(chef_values)) {
  sim_results <- simulate_day(chef_values[i])
  wait_times[i] <- sim_results$avg_wait_time
}

# Create a table
chef_table <- data.frame(
  "Number of Chefs" = chef_values,
  "Average Wait Time" = wait_times
)

names(chef_table)[1] <- "Number of Chefs"
names(chef_table)[2] <- "Average Wait Time (Hours)"

kable(chef_table, format = "markdown", caption = "Average Wait Times predicted by Chef Number")
```
As the number of chefs $L$ increases, the service rate $\lambda_S$ multiplies accordingly, which enhances the restaurant's ability to manage customer queues effectively. This directly reduces the average wait time, thereby potentially increasing customer satisfaction and retention. However, diminishing returns are likely as the number of chefs increases beyond the point where service rate far exceeds the arrival rate of new customers, leading to unnecessary labor costs without corresponding increases in customer throughput. I would say, at this stage of analysis, we reach that point past 6 chefs.


```{r Optimal Chefs, echo=FALSE, warning=FALSE, message=FALSE}

library(queueing)

simulate_day <- function(L) {
  lambda_A <- 10  
  num_tables <- 5  
  lambda_S <- 3 * L  
  opening_hour <- 10  
  closing_hour <- 22

  Q_mm_c <- NewInput.MMC(lambda = lambda_A, mu = lambda_S, c = num_tables, n = 1000)
  result <- QueueingModel(Q_mm_c)

  # Calculate hourly revenue based on the minimum of customer arrivals or table capacity
  hourly_revenue <- min(lambda_A, lambda_S * num_tables) * 50  # $50 per customer
  daily_revenue <- hourly_revenue * (closing_hour - opening_hour)  # Calculate hours of operation

  # Calculate daily wages for chefs
  daily_wages <- L * 40 * (closing_hour - opening_hour)  # $40 per hour per chef

  # Calculate profits
  profits <- daily_revenue - daily_wages

  # Get average wait time in minutes
  avg_wait_time <- result$Lq / lambda_A * 60

  return(list(profits = profits, avg_wait_time = avg_wait_time, revenue = daily_revenue, wages = daily_wages))
}

# Function to find optimal number of chefs
optimize_chefs <- function(max_chefs = 10) {
  results <- data.frame(Chefs = integer(), Profits = numeric(), AvgWaitTime = numeric(), Revenue = numeric(), Wages = numeric())
  
  for (L in 1:max_chefs) {
    sim_results <- simulate_day(L)
    results[nrow(results) + 1, ] <- c(L, sim_results$profits, sim_results$avg_wait_time, sim_results$revenue, sim_results$wages)
  }
  
  optimal_chefs <- results[which.max(results$Profits), ]
  return(list(optimal = optimal_chefs, details = results))
}

# Execute the optimization function and print results
optimization_results <- optimize_chefs()


kable(optimization_results$details, format = "markdown", caption = "Optimal Number of Chefs")

```

From a business perspective, the optimal number of chefs $L$ should be selected to balance the cost of hiring additional staff against the generated revenue from serving more customers efficiently. The profits calculated suggest that maximizing profits involves not merely increasing the number of chefs to reduce wait times but also considering the point where additional chefs no longer contribute to significant increases in customer throughput or revenue. At this stage of analysis, one could go with two perspectives, pure profit or customer experience. For the purposes of pure profit, it is clear that one chef takes the cake. Yet, the customer wait time for one chef is exponentially higher than all the others, detailing the stark need for more than one chef. Two chefs gets you the perfect balance of profit and wait time. $\frac{9}{100}$ of an hour is only 5.4 minutes, which is a very reasonable time to wait for food.

```{r}
library(queueing)

# Function to simulate a day and calculate downtime
simulate_day_with_downtime <- function(L) {
  lambda_A <- 10  # Customers arriving per hour
  num_tables <- 5  # Number of available tables
  lambda_S <- 3 * L  # Service rate, dependent on number of chefs
  
  # Queueing model for an M/M/c system
  Q_mm_c <- NewInput.MMC(lambda = lambda_A, mu = lambda_S, c = num_tables, n = 1000)
  result <- QueueingModel(Q_mm_c)
  
  # Calculate downtime as the proportion of time when chefs are underutilized
  expected_customers_served_per_hour <- min(lambda_A, lambda_S)
  expected_idle_chefs_per_hour <- max(0, L - expected_customers_served_per_hour / 3)
  downtime_proportion <- expected_idle_chefs_per_hour / L
  
  # Check if utilization is set in the result, otherwise set to 0
  utilization <- ifelse(is.null(result$U), 0, result$U)
  
  return(list(downtime_proportion = downtime_proportion, utilization = utilization))
}

# Simulating different numbers of chefs and recording downtime
chef_values <- 1:10
downtime_results <- numeric(length(chef_values))
utilizations <- numeric(length(chef_values))

for (i in seq_along(chef_values)) {
  sim_results <- simulate_day_with_downtime(chef_values[i])
  downtime_results[i] <- sim_results$downtime_proportion
  utilizations[i] <- sim_results$utilization
}

# Create a table of results
downtime_table <- data.frame(
  "Number of Chefs" = chef_values,
  "Downtime Proportion" = downtime_results)

names(downtime_table)[1] <- "Number of Chefs"
names(downtime_table)[2] <- "Downtime Proportion"

# Use kable from knitr to create a nicely formatted table
library(knitr)
kable(downtime_table, format = "markdown", caption = "Proportion of Downtime and Utilization by Number of Chefs")

names(downtime_table)[1] <- "Number of Chefs"
names(downtime_table)[2] <- "Downtime Proportion"

```
One final thing that is essential to factor in is downtime for the number of chefs we possess. Without proper downtime, no cleaning, breaks, or just time to catch a breath can occur. This will lead to decreased levels of customer satisfaction when they see dirty counter tops or encounter peeved chefs. With downtime as an added variable of interest, we see that 2 chefs is no longer the optimal number. Only having two chefs allows for no downtime. *Four chefs* seems to be the sweet spot, with 10 minutes of downtime (for all chefs collectively) per hour. That amount also maximizes profits the most and ensures a very speedy customer wait time. 


## Discussions & Limitations

## Conclusion

## References

#### Lab 10

Rossetti, R. (2024, April 4). Sakai.duke.edu. <https://sakai.duke.edu/access/content/group/2e691ccc-405a-4982-bf81-42137192402e/Labs/lab10_sol.html>

#### Simulation of Exponential Distribution using R

Hernández, C. (2020, September 29). Simulation of Exponential Distribution using R. RPubs. <https://rpubs.com/carlosehernandezr/Exponential-Distribution#>:\~:text=The%20exponential%20distribution%20can%20be,deviation%20is%20also%201%2Flambda
